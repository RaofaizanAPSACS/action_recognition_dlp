{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1900619,"sourceType":"datasetVersion","datasetId":1132746}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Action Recognition Detection using CNN + LSTM neural netowrk","metadata":{"id":"-x-x2hhlOKkl"}},{"cell_type":"markdown","source":"## Flowchart","metadata":{"id":"L8w5vbqMObaw"}},{"cell_type":"markdown","source":"The method consists of extracting a set of frames belonging to the video, sending them to a pretrained network called VGG16, obtaining the output of one of its final layers and from these outputs train another network architecture with a type of special neurons called LSTM. These neurons have memory and are able to analyze the temporal information of the video, if at any time they detect violence, it will be classified as a violent video.\n\n\n\n","metadata":{"id":"6kz7yE3kOgMk"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"UVXa7uMnOlkp"}},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport os\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\n# import download\nfrom random import shuffle\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Activation\nimport sys\nimport h5py","metadata":{"id":"oDfDnlliPMd-","outputId":"7582c7ed-2ba2-4164-e043-d0c8d31ba19f","execution":{"iopub.status.busy":"2024-05-09T09:01:14.745654Z","iopub.execute_input":"2024-05-09T09:01:14.746084Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-09 09:01:17.084285: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 09:01:17.084406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 09:01:17.231151: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_progress(count, max_count):\n    # Percentage completion.\n    pct_complete = count / max_count\n\n    # Status-message. Note the \\r which means the line should\n    # overwrite itself.\n    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n\n    # Print it.\n    sys.stdout.write(msg)\n    sys.stdout.flush()","metadata":{"id":"qnafWmS7P3CG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_dir = \"/kaggle/input/hockey-fight-vidoes/data\"\nimport os\nfiles = os.listdir(in_dir)\nlen(files)","metadata":{"id":"RiRKgwBgP-NY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = 224\nimg_size_touple = (img_size, img_size)\nnum_channels = 3\nimg_size_flat = img_size * img_size * num_channels\nnum_classes = 2\n_num_files_train = 1\n_images_per_file = 20\n_num_images_train = _num_files_train * _images_per_file\nvideo_exts = \".avi\"","metadata":{"id":"SXTNEj6SRLZZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper-function for getting video frames","metadata":{"id":"Wodq7EaSRSS8"}},{"cell_type":"code","source":"def get_frames(current_dir, file_name):\n    \n    in_file = os.path.join(current_dir, file_name)\n    \n    images = []\n    \n    vidcap = cv2.VideoCapture(in_file)\n    \n    success,image = vidcap.read()\n        \n    count = 0\n\n    while count<_images_per_file:\n                \n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n                                 interpolation=cv2.INTER_CUBIC)\n    \n        images.append(res)\n    \n        success,image = vidcap.read()\n    \n        count += 1\n        \n    resul = np.array(images)\n    \n    resul = (resul / 255.).astype(np.float16)\n        \n    return resul","metadata":{"id":"eu9c4a-3RVkO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper function to get the names of the data downloaded and label it","metadata":{"id":"tLCjYFBtRZb-"}},{"cell_type":"code","source":"def label_video_names(in_dir):\n    \n    # list containing video names\n    names = []\n    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n    labels = []\n    \n    \n    for current_dir, dir_names,file_names in os.walk(in_dir):\n        for file_name in file_names:\n            \n            if file_name[0:2] == 'fi':\n                labels.append([1,0])\n                names.append(file_name)\n            elif file_name[0:2] == 'no':\n                labels.append([0,1])\n                names.append(file_name)\n                   \n    c = list(zip(names,labels))\n    # Suffle the data (names and labels)\n    shuffle(c)\n    \n    names, labels = zip(*c)\n            \n    return names, labels","metadata":{"id":"Qiv5NIJjRbIA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a video frame to see if data is correct","metadata":{"id":"t3KW2kfgReKn"}},{"cell_type":"code","source":"names, labels = label_video_names(in_dir)","metadata":{"id":"dIsaAgcyRfIx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names[12]","metadata":{"id":"xUfZO-0BRj0f","outputId":"e2b913dc-c39f-4d5f-a0d6-da1273692fbc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = get_frames(in_dir, names[12])","metadata":{"id":"EqBi8z6rRoMW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visible_frame = (frames*255).astype('uint8')","metadata":{"id":"9ihSA_ogRsNU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[3])","metadata":{"id":"PM1kNhaHRvSv","outputId":"e2a6d324-cb62-42d2-dd8f-1cf1c464d168","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(visible_frame[15])","metadata":{"id":"_gVWtYPvR8n2","outputId":"f9858ffa-3981-4b85-9c4d-62057bb8972c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Trained Model: VGG16","metadata":{"id":"sF1QieG5SANp"}},{"cell_type":"code","source":"image_model = VGG16(include_top=True, weights='imagenet')","metadata":{"id":"MjRN6oE4SC81","outputId":"1a9fbd16-9741-406d-b4fd-e3a20cd34a0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_model.summary()\n","metadata":{"id":"ud7OU0t7SQPi","outputId":"20718d3a-c1d3-4a06-d6f5-de3c465311df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will use the output of the layer prior to the final\n# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\ntransfer_layer = image_model.get_layer('fc2')\n\nimage_model_transfer = Model(inputs=image_model.input,\n                             outputs=transfer_layer.output)\n\ntransfer_values_size = K.int_shape(transfer_layer.output)[1]\n\n\nprint(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n\nprint(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)","metadata":{"id":"4YWFA-2tSdfB","outputId":"55dcb0c7-2b8a-4420-e25e-381152197a81","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to process 20 video frames through VGG16 and get transfer values","metadata":{"id":"1ghum6s2Si1n"}},{"cell_type":"code","source":"def get_transfer_values(current_dir, file_name):\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n    image_batch = get_frames(current_dir, file_name)\n      \n    # Pre-allocate output-array for transfer-values.\n    # Note that we use 16-bit floating-points to save memory.\n    shape = (_images_per_file, transfer_values_size)\n    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n\n    transfer_values = \\\n            image_model_transfer.predict(image_batch)\n            \n    return transfer_values","metadata":{"id":"-AdxYAtiSlLF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator that process one video through VGG16 each function call","metadata":{"id":"HGdRIG6oSooG"}},{"cell_type":"code","source":"def proces_transfer(vid_names, in_dir, labels):\n    \n    count = 0\n    \n    tam = len(vid_names)\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    while count<tam:\n        \n        video_name = vid_names[count]\n        \n        image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n        image_batch = get_frames(in_dir, video_name)\n        \n         # Note that we use 16-bit floating-points to save memory.\n        shape = (_images_per_file, transfer_values_size)\n        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n        \n        transfer_values = \\\n            image_model_transfer.predict(image_batch)\n         \n        labels1 = labels[count]\n        \n        aux = np.ones([20,2])\n        \n        labelss = labels1*aux\n        \n        yield transfer_values, labelss\n        \n        count+=1","metadata":{"id":"D2BvaY3eSpSH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_files(n_files):\n    \n    gen = proces_transfer(names_training, in_dir, labels_training)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('action_cached.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"tvU53ypSSvL0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_files_test(n_files):\n    \n    gen = proces_transfer(names_test, in_dir, labels_test)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('action_cachedvalidation.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","metadata":{"id":"8nK8uFExS0nX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the dataset into training set and test set\nWe are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy.","metadata":{"id":"R9axnZ8dS64T"}},{"cell_type":"code","source":"training_set = int(len(names)*0.8)\ntest_set = int(len(names)*0.2)\n\nnames_training = names[0:training_set]\nnames_test = names[training_set:]\n\nlabels_training = labels[0:training_set]\nlabels_test = labels[training_set:]","metadata":{"id":"wjne3svdS9Y3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_files(training_set)","metadata":{"id":"pyO9WP-6TER4","outputId":"94209006-bfb9-470f-d418-cd2acfbdb12d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_files_test(test_set)","metadata":{"id":"0ThoefXUXPrS","outputId":"9f4cdc1e-4bfb-4ab6-81c1-05abadc1239f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_alldata_training():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('action_cached.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Ez0blP2z0CsF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_alldata_test():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('action_cachedvalidation.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","metadata":{"id":"Dne2XaQ90MGk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data, target = process_alldata_training()","metadata":{"id":"phyhoYc67VW8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test, target_test = process_alldata_test()","metadata":{"id":"dJpXHvEg7Xhc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recurrent Neural Network","metadata":{"id":"Qjaawyqj0arq"}},{"cell_type":"markdown","source":"### Define LSTM architecture","metadata":{"id":"BsIrL3ix4Edp"}},{"cell_type":"markdown","source":"When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n","metadata":{"id":"qFiSjMaC4Q1c"}},{"cell_type":"markdown","source":"The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n","metadata":{"id":"HAgSUeVu58N_"}},{"cell_type":"code","source":"chunk_size = 4096\nn_chunks = 20\nrnn_size = 512\n\nmodel = Sequential()\nmodel.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])","metadata":{"id":"XWABZ91b6f7l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training\n","metadata":{"id":"bVonOPYW7F_b"}},{"cell_type":"code","source":"epoch = 200\nbatchS = 500\n\nhistory = model.fit(np.array(data[0:720]), np.array(target[0:720]), epochs=epoch,\n                    validation_data=(np.array(data[720:]), np.array(target[720:])), \n                    batch_size=batchS, verbose=2)","metadata":{"id":"iRZlW4ZV_ygS","outputId":"c223a55f-8ccd-4e5a-f5d4-dc6a1daf2d0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the model","metadata":{"id":"BVJCgz3bA4A3"}},{"cell_type":"code","source":"result = model.evaluate(np.array(data_test), np.array(target_test))","metadata":{"id":"VDXFFy6zBG3X","outputId":"6ac389f9-6400-401d-dfe7-c3e90ce8a0e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Print the model accuracy","metadata":{"id":"8YG-bPW4BL6U"}},{"cell_type":"code","source":"for name, value in zip(model.metrics_names, result):\n    print(name, value)","metadata":{"id":"wBV2t2Q6BOt9","outputId":"1dadcb49-b517-4cdc-9624-cb76c529b457","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path.eps', format='eps', dpi=1000)\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path1.eps', format='eps', dpi=1000)\nplt.show()","metadata":{"id":"zO6YSbxgBZ3f","outputId":"f8a99f0c-2cf2-4228-f6ac-9083c39443fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the LSTM MODEL","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/lstm_model_v1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.applications import VGG16\n\n# Initialize the VGG16 model\nvgg_model = VGG16(include_top=True, weights='imagenet')\nvgg_model_transfer = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('fc2').output)\n\ndef load_and_process_video(video_path, img_size, frames_count):\n    vidcap = cv2.VideoCapture(video_path)\n    frames = []\n    success, image = vidcap.read()\n    count = 0\n\n    while success and count < frames_count:\n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        resized_img = cv2.resize(RGB_img, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n        frames.append(resized_img)\n        success, image = vidcap.read()\n        count += 1\n\n    # Normalize and return the frames as a batch\n    frames_array = np.array(frames) / 255.0\n    return frames_array\n\ndef get_transfer_values(frames_batch):\n    # Predict the transfer values with VGG16\n    transfer_values = vgg_model_transfer.predict(frames_batch)\n    return transfer_values\n\ndef predict_video(model, video_path, img_size, frames_count):\n    # Load and preprocess the video\n    frames_batch = load_and_process_video(video_path, img_size, frames_count)\n    # Get the transfer values\n    transfer_values = get_transfer_values(frames_batch)\n    # Expand dimensions to match the input shape of LSTM model\n    transfer_values = np.expand_dims(transfer_values, axis=0)\n    # Make a prediction\n    predictions = model.predict(transfer_values)\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T03:54:45.163062Z","iopub.execute_input":"2024-05-09T03:54:45.163796Z","iopub.status.idle":"2024-05-09T03:54:47.310580Z","shell.execute_reply.started":"2024-05-09T03:54:45.163763Z","shell.execute_reply":"2024-05-09T03:54:47.309710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Parameters\nvideo_path = '/kaggle/input/hockey-fight-vidoes/data/fi102_xvid.avi'\nimg_size = 224\nframes_count = 20\n\n# Prediction\npredictions = predict_video(model, video_path, img_size, frames_count)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T03:54:51.449172Z","iopub.execute_input":"2024-05-09T03:54:51.449836Z","iopub.status.idle":"2024-05-09T03:54:52.188152Z","shell.execute_reply.started":"2024-05-09T03:54:51.449801Z","shell.execute_reply":"2024-05-09T03:54:52.187257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-09T03:54:58.981862Z","iopub.execute_input":"2024-05-09T03:54:58.982561Z","iopub.status.idle":"2024-05-09T03:54:58.989442Z","shell.execute_reply.started":"2024-05-09T03:54:58.982524Z","shell.execute_reply":"2024-05-09T03:54:58.988565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.applications import VGG16\n\n# Initialize the VGG16 model\nvgg_model = VGG16(include_top=True, weights=\"imagenet\")\nvgg_model_transfer = Model(\n    inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output\n)\n\n\ndef load_and_process_video(video_path, img_size, frames_count):\n    vidcap = cv2.VideoCapture(video_path)\n    frames = []\n    success, image = vidcap.read()\n    count = 0\n\n    while success and count < frames_count:\n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        resized_img = cv2.resize(\n            RGB_img, (img_size, img_size), interpolation=cv2.INTER_CUBIC\n        )\n        frames.append(resized_img)\n        success, image = vidcap.read()\n        count += 1\n\n    # Normalize and return the frames as a batch\n    frames_array = np.array(frames) / 255.0\n    return frames_array\n\n\ndef get_transfer_values(frames_batch):\n    # Predict the transfer values with VGG16\n    transfer_values = vgg_model_transfer.predict(frames_batch)\n    return transfer_values\n\n\ndef predict_video(model, video_path, img_size, frames_count):\n    # Load and preprocess the video\n    frames_batch = load_and_process_video(video_path, img_size, frames_count)\n    # Get the transfer values\n    transfer_values = get_transfer_values(frames_batch)\n    # Expand dimensions to match the input shape of LSTM model\n    transfer_values = np.expand_dims(transfer_values, axis=0)\n    # Make a prediction\n    predictions = model.predict(transfer_values)\n    return predictions\n\n# if __name__ == \"__main___\":\nprint(\"working...\")\nvideo_path = './videos/no277_xvid.avi'\nimg_size = 224\nframes_count = 20\nmodel = load_model('./models/lstm_model_v1.h5')\n\npredictions = predict_video(model, video_path, img_size, frames_count)\nprint(predictions)","metadata":{},"execution_count":null,"outputs":[]}]}